{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get COVID Related Wikipedia Pages\n",
    "\n",
    "This notebook retrieves COVID-19 related wikiepdia pages from wikimedia dumps which created before set up time.  \n",
    "You can utilize below codes to get wikipedia pages from dump with other topic.\n",
    "\n",
    "**Set up values**  \n",
    "* Target Language (2-letter language code needed)\n",
    "* Seed QIDs (Target Wikidata articles's QID)\n",
    "* Sample Date\n",
    "\n",
    "**Procedure of retrieving**  \n",
    "1. Wikidata Pageids(in Pagelinks)\n",
    "2. Wikidata QIDs\n",
    "3. For each languages\n",
    "   - Get Wikipedia Pageids\n",
    "   - Get Page Title\n",
    "   - Set Category of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,gzip,glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def download_dumps(languages,date):\n",
    "    if not os.path.exists(\"/dumps\"):\n",
    "        os.mkdir(\"dumps\")\n",
    "    destination_folder = os.path.abspath(\"/dumps\")\n",
    "\n",
    "    #Wikidata Pagelinks & Pages - SQL File\n",
    "    pagelinks_url = \"https://dumps.wikimedia.org/wikidatawiki/{}/wikidatawiki-{}-pagelinks.sql.gz\".format(date,date)\n",
    "    pages_url = \"https://dumps.wikimedia.org/wikidatawiki/{}/wikidatawiki-{}-page.sql.gz\".format(date,date)\n",
    "\n",
    "    wget.download(pagelinks_url,out = destination_folder)\n",
    "    wget.download(pages_url,out = destination_folder)\n",
    "\n",
    "    #Wikipedia - SQL File\n",
    "    for language in languages:\n",
    "        url = \"https://dumps.wikimedia.org/{}wiki/{}/{}wiki-{}-page_props.sql.gz\".format(language,date,language,date)\n",
    "        wget.download(url,out = destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Target Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Set (Nov.20th)\n",
    "date = '20201120'\n",
    "\n",
    "#Target Language \n",
    "lang = ['en','ko','ar','de','es','fr','it','ja','pt','zh','ru']\n",
    "\n",
    "#Target Seeds of Wikidata Article\n",
    "seeds = ['Q81068910','Q82069695','Q84263196']\n",
    "\n",
    "#download dumps\n",
    "download_dumps(lang,date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Wikidata Pageid from pagelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get seeds from pagelink.sql file (Alternative : \"|\".join(seeds))\n",
    "command = 'grep -E \"{}\" wikidatawiki-{}-pagelinks.sql > COVID_item_zgrepmatch.txt'.format(\"|\".join(seeds),date)\n",
    "!$command\n",
    "\n",
    "#Get wikidata page ids which linked to COVID-19 seed items\n",
    "wikidataPagesIds = []\n",
    "f = open('COVID_item_zgrepmatch.txt')\n",
    "for l  in f:\n",
    "    for s in seeds:\n",
    "        wikidataPagesIds.extend(re.findall(\"\\(([0-9]+),0,\\'%s\\'\" % s,l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Wikidata Article QIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB Connection : Before Running below parts, you should import downloaded dump sql files to DB\n",
    "import pymysql\n",
    "\n",
    "#Get related QIDs from wikidata page ids (dump filename : wikidata_page.sql.gz)\n",
    "conn = pymysql.connect(host='node200',user='jaehyeon',password='', db='testdb',charset='utf8')\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "sql = \"SELECT page_id,page_title FROM page WHERE page_id IN ({})\".format(','.join(wikidataPagesIds))\n",
    "cursor.execute(sql)\n",
    "COVID_related_QIDs = [page['page_title'].decode() for page in cursor.fetchall() if page['page_title'].decode('utf8')[0]=='Q'] \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get Wikipedia PageIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve pageid for each languages (dump filename : wikipedia_page_prop.sql.gz)\n",
    "conn = pymysql.connect(host='node200',user='jaehyeon',password='', db='testdb',charset='utf8')\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "wikidata_dict = {}\n",
    "sql_QIDs = ','.join(\"'{0}'\".format(QID) for QID in COVID_related_QIDs)\n",
    "for lan in lang:\n",
    "    sql = f\"SELECT pp_page,pp_propname,pp_value FROM {lan}_page_props WHERE pp_value IN ({sql_QIDs}) AND pp_propname = 'wikibase_item'\"\n",
    "    cursor.execute(sql)\n",
    "    id_dict = {}\n",
    "    for page in cursor.fetchall():\n",
    "        id_dict[page['pp_value'].decode()] = {'QID' : page['pp_value'].decode(),\n",
    "                           'PageID' : page['pp_page'],\n",
    "                           'Language' : lan+\"wiki\"} \n",
    "    wikidata_dict[lan] = id_dict\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get Wikipedia Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    qids = list(wikidata_dict[l].keys())\n",
    "    seg_qid_list = [qids[i * n:(i + 1) * n] for i in range((len(qids) + n - 1) // n )] \n",
    "    for ids in seg_qid_list:\n",
    "        url_ids = '|'.join(ids)\n",
    "        url = \"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&props=sitelinks&ids={}&sitefilter={}wiki\".format(url_ids,l)\n",
    "        entities = requests.get(url).json().get('entities')\n",
    "        print(entitites)\n",
    "        for k,v in entities.items():\n",
    "            item = entities.get(k)\n",
    "            if item:\n",
    "                sitelink = item.get('sitelinks')\n",
    "                if sitelink:\n",
    "                    title = sitelink.get(l+'wiki').get('title')\n",
    "                    if title:\n",
    "                        wikidata_dict[l][k]['Title'] = title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Set Category of Articles\n",
    "Region & Human Category pages are selected from Wikidata \"instance of\" label. If label is \"human\" then category is \"Human\". Else if label is \"outbreak\", category is \"Region\". Label data can be collected from wikidata query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get Wikidata label data from wikimedia api\n",
    "def get_label_qid(target_label,qid):\n",
    "    url = \"https://www.wikidata.org/w/api.php?action=wbgetentities&ids=%s&format=json&languages=en\"%(qid)\n",
    "    entities = requests.get(url).json().get('entities')\n",
    "    return entities[qid]['claims']['P31'][0]['mainsnak']['datavalue']['value']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lang:\n",
    "    for k, v in wikidata_dict[l].items():\n",
    "        label == get_label_qid('P31',qid) # P31 : instance of\n",
    "        if k in seeds:\n",
    "            v['category'] = 'Bio-Med'\n",
    "        elif label == 'Q3241045': # Q3241045 : disease outbreak\n",
    "            v['category'] = 'Region'\n",
    "        elif label == 'Q5': # Q5 : human\n",
    "            v['category'] = 'Human'\n",
    "        else:\n",
    "            v['category'] = 'Others'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CSV Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lang:\n",
    "    name = \"clean_list_{}/wikipedia_list_cleaned_{}_{}.csv\".format(date,date,l)\n",
    "    with open(name,'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(('QID','page id','wiki_db','item','category'))\n",
    "        for key, value in wikidata_dict[l].items():\n",
    "            writer.writerow(list(value.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
