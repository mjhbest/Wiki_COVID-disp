{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,gzip,glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import mwxml\n",
    "import bz2\n",
    "import itertools\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Current Time (Nov.20th)\n",
    "date = '20201120'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language & Seeds\n",
    "lang = ['en','ko','ar','de','es','fr','it','ja','pt','zh','ru']\n",
    "seeds = ['Q81068910','Q82069695','Q84263196']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get seeds from pagelink.sql file\n",
    "!grep -E \"Q81068910|Q82069695|Q84263196\" wikidatawiki-20201120-pagelinks.sql > COVID_item_zgrepmatch.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get wikidata page ids which linked to COVID-19 seed items\n",
    "wikidataPagesIds = []\n",
    "seeds = ['Q81068910','Q82069695','Q84263196']\n",
    "f = open('COVID_item_zgrepmatch.txt')\n",
    "for l  in f:\n",
    "    for s in seeds:\n",
    "        wikidataPagesIds.extend(re.findall(\"\\(([0-9]+),0,\\'%s\\'\" % s,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikidata_QIDs.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(COVID_related_QIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MySQL Connection\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Article QIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get related QIDs from wikidata page ids (dump filename : wikidata_page.sql.gz)\n",
    "conn = pymysql.connect(host='node200',user='jaehyeon',password='', db='testdb',charset='utf8')\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "sql = \"SELECT page_id,page_title FROM page WHERE page_id IN ({})\".format(','.join(wikidataPagesIds))\n",
    "cursor.execute(sql)\n",
    "COVID_related_QIDs = [page['page_title'].decode() for page in cursor.fetchall() if page['page_title'].decode('utf8')[0]=='Q'] \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Wikipedia PageIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve pageid for each languages (dump filename : wikipedia_page_prop.sql.gz)\n",
    "conn = pymysql.connect(host='node200',user='jaehyeon',password='', db='testdb',charset='utf8')\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "wikidata_dict = {}\n",
    "sql_QIDs = ','.join(\"'{0}'\".format(QID) for QID in COVID_related_QIDs)\n",
    "for lan in lang:\n",
    "    sql = f\"SELECT pp_page,pp_propname,pp_value FROM {lan}_page_props WHERE pp_value IN ({sql_QIDs}) AND pp_propname = 'wikibase_item'\"\n",
    "    cursor.execute(sql)\n",
    "    id_dict = {}\n",
    "    for page in cursor.fetchall():\n",
    "        id_dict[page['pp_value'].decode()] = {'QID' : page['pp_value'].decode(),\n",
    "                           'PageID' : page['pp_page'],\n",
    "                           'Language' : lan+\"wiki\"} \n",
    "    wikidata_dict[lan] = id_dict\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Wikipedia Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    qids = list(wikidata_dict[l].keys())\n",
    "    seg_qid_list = [qids[i * n:(i + 1) * n] for i in range((len(qids) + n - 1) // n )] \n",
    "    for ids in seg_qid_list:\n",
    "        url_ids = '|'.join(ids)\n",
    "        url = \"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&props=sitelinks&ids={}&sitefilter={}wiki\".format(url_ids,l)\n",
    "        entities = requests.get(url).json().get('entities')\n",
    "        print(entitites)\n",
    "        for k,v in entities.items():\n",
    "            item = entities.get(k)\n",
    "            if item:\n",
    "                sitelink = item.get('sitelinks')\n",
    "                if sitelink:\n",
    "                    title = sitelink.get(l+'wiki').get('title')\n",
    "                    if title:\n",
    "                        wikidata_dict[l][k]['Title'] = title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region & Human Category pages are selected from Wikidata \"instance of\" label. If label is \"human\" then category is \"Human\". Else if label is \"outbreak\", category is \"Region\". Label data can be collected from wikidata query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Region Categories\n",
    "region_qids = []\n",
    "f = open('region.csv', 'r', encoding='utf-8')\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    region_qids.append(line[0].split('/')[-1])\n",
    "f.close()    \n",
    "region_qids = region_qids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Human Categories\n",
    "human_qids = []\n",
    "f = open('human.csv', 'r', encoding='utf-8')\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    human_qids.append(line[0].split('/')[-1])\n",
    "f.close()    \n",
    "region_qids = region_qids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lang:\n",
    "    for k, v in wikidata_dict[l].items():\n",
    "        if k in ['Q81068910','Q82069695','Q84263196']:\n",
    "            v['category'] = 'Bio-Med'\n",
    "        elif k in region_qids:\n",
    "            v['category'] = 'Region'\n",
    "        elif k in human_qids:\n",
    "            v['category'] = 'Human'\n",
    "        else:\n",
    "            v['category'] = 'Others'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lang:\n",
    "    name = \"wikipedia_list_cleaned_1120_{}.csv\".format(l)\n",
    "    with open(name,'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(('QID','page id','wiki_db','item','category'))\n",
    "        for key, value in wikidata_dict[l].items():\n",
    "            writer.writerow(list(value.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lang:\n",
    "    print(l,len(wikidata_dict[l].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1120_data.json','w') as f:\n",
    "    json.dump(wikidata_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Common List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = [list(wikidata_dict[l].keys()) for l in lang]\n",
    "common_list = list(set(qids[0]).intersection(*qids))\n",
    "\n",
    "name = \"wikipedia_common_list_1120.csv\"\n",
    "with open(name,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(common_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
