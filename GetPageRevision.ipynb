{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Edit History\n",
    "This notebook takes wikimedia xml dump files and COVID related pages per language as input files and retrieves all the edit history which have done after fixed date (ex 20200101).\n",
    "\n",
    "Dump URL : https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "Retrieved data is as follows :\n",
    "* Revision ID\n",
    "* Revision Timestamp\n",
    "* Size of Article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Setting\n",
    "\n",
    "* Download history dumps\n",
    "* Set values for data (Date, Target Languages, Start Date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wiki_dump_parser as parser\n",
    "import mwxml\n",
    "import wget\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from mwtypes import Timestamp\n",
    "\n",
    "def download_history_dumps(date,languages): \n",
    "    if os.path.exists(\"/dumps\")\n",
    "        destination_folder = os.path.abspath(\"/dumps\")\n",
    "    else:\n",
    "        os.mkdir(\"dumps\")\n",
    "        \n",
    "    for language in languages:\n",
    "        url = \"https://dumps.wikimedia.org/{}wiki/{}/{}wiki-{}-stub-meta-history.xml.gz\".format(language,date,language,date)\n",
    "        wget.download(url,out = destination_folder)\n",
    "        \n",
    "\n",
    "languages = ['en','ko','ar','de','es','fr','it','ja','pt','zh','ru']\n",
    "date = \"20201120\"\n",
    "start_date = Timestamp(\"2020-01-01T00:00:00Z\")\n",
    "\n",
    "#Download meta-history dump\n",
    "download_history_dumps(date,languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Revision Data\n",
    "\n",
    "The code below takes a considerable amount of time.  \n",
    "You can improve effeciency by using 'Regular Expression' to extract essential pages data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load target article's pageid list\n",
    "lang_list = {}\n",
    "for language in languages:\n",
    "    df = pd.read_csv('clean_list_%s/clean_related_list_%s.csv'%(date,language))\n",
    "    lang_list[language] = {\n",
    "        'pageid' : df['PageID'].tolist(),\n",
    "        'title' : df['Title']\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": []
    }
   ],
   "source": [
    "#retrieve revision datas from dump\n",
    "for language in languages:\n",
    "    df = pd.DataFrame(columns=['pageid','time','size','rev_id'])\n",
    "    idx = 0\n",
    "    dump = mwxml.Dump.from_file(gzip.open(\"history_dumps/%swiki-%s-stub-meta-history.xml.gz\"%(date,date,language,date),'rb'))\n",
    "    for page in dump:\n",
    "        if page.id in lang_list[language]['pageid']:\n",
    "            for revision in page:\n",
    "                if revision.timestamp > start_date:\n",
    "                    df.loc[i] = [page.id,revision.timestamp,revision.bytes,revision.id]\n",
    "    df.to_csv('wikiedit/wikiedits_%s_%s'%(date,language),index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
